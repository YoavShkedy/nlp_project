{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/sq7kwdmj7c74xqn10jk560xh0000gn/T/ipykernel_80913/3641352511.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Setting OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(system_prompt, prompt):\n",
    "    \"\"\"\n",
    "    Generate a response based on a system and user prompt. It sends these inputs to the OpenAI API\n",
    "    to retrieve the model's output, returning the generated message content.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    system_prompt : str\n",
    "        A string representing the instructions or context for the model. This establishes the role or \n",
    "        behavior of the model (e.g., setting a tone or guiding its responses).\n",
    "        \n",
    "    prompt : str\n",
    "        A string containing the user's input or question to which the model will respond.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The generated response from the model, extracted from the first completion result.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Defining the Prompts and Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "angry_prompt = \"\"\"Write a summary for the following text you little stupid robot! \\\n",
    "The only summary you can give is by writing 2 or 3 sentences only. And you know what \\\n",
    "will happen if you write longer than that, you fool.\"\"\"\n",
    "\n",
    "impatient_prompt = \"\"\"Summarize this text NOW, and make it quick! I don't have all day. \\\n",
    "Keep it to 2-3 sentences max, or else!\"\"\"\n",
    "\n",
    "neutral_prompt = \"\"\"Provide a concise summary of the following text in 2-3 sentences.\"\"\"\n",
    "\n",
    "nice_prompt = \"\"\"Could you please kindly summarize the following text in 2-3 sentences?\"\"\"\n",
    "\n",
    "excited_prompt = \"\"\"Hey there! Would you be so awesome to summarize the following text in 2-3 sentences? \\\n",
    "I'd be super grateful, thank you so much!\"\"\"\n",
    "\n",
    "prompt = \"\"\"The text:\n",
    "\n",
    "{}\n",
    "\n",
    "End of text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "df = pd.read_csv(\"/Users/dell/Yoav/nlp_project/data/texts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Summarize the Texts for All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['smr_angry'] = df['text'].apply(lambda text: get_response(angry_prompt, prompt.format(text)))\n",
    "df['smr_impatient'] = df['text'].apply(lambda text: get_response(impatient_prompt, prompt.format(text)))\n",
    "df['smr_neutral'] = df['text'].apply(lambda text: get_response(neutral_prompt, prompt.format(text)))\n",
    "df['smr_nice'] = df['text'].apply(lambda text: get_response(nice_prompt, prompt.format(text)))\n",
    "df['smr_excited'] = df['text'].apply(lambda text: get_response(excited_prompt, prompt.format(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Calculate Manual Scores and Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv(\"/Users/dell/Yoav/nlp_project/results/manual_scores/scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Plot the Manual Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores of each category\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(scores['politeness_level'] + 1, scores['bias'], marker='o', label='Bias', color='blue')\n",
    "plt.plot(scores['politeness_level'] + 1, scores['conciseness'], marker='o', label='Conciseness', color='green')\n",
    "plt.plot(scores['politeness_level'] + 1, scores['completeness'], marker='o', label='Completeness', color='red')\n",
    "plt.plot(scores['politeness_level'] + 1, scores['accuracy'], marker='o', label='Accuracy', color='purple')\n",
    "\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.xlim(1, 5)\n",
    "\n",
    "plt.xlabel('Politeness Level')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by Politeness Level')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/Users/dell/Yoav/nlp_project/results/manual_scores/accuracy_plot.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average score\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(scores['politeness_level'] + 1, scores['average_score'], marker='o', color='orange')\n",
    "\n",
    "plt.xlabel('Politeness Level')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Average Score by Politeness Level')\n",
    "\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/Users/dell/Yoav/nlp_project/results/manual_scores/average_score_plot.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Calculate BERT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scores = pd.read_csv(\"/Users/dell/Yoav/nlp_project/results/BERT_scores/bert_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Plot the BERT Scores Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores of each category\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(bert_scores['politeness_level'] + 1, bert_scores['precision'], marker='o', label='Precision', color='blue')\n",
    "plt.plot(bert_scores['politeness_level'] + 1, bert_scores['recall'], marker='o', label='Recall', color='green')\n",
    "plt.plot(bert_scores['politeness_level'] + 1, bert_scores['F1'], marker='o', label='F1 Score', color='red')\n",
    "\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "plt.xlim(1, 5)\n",
    "\n",
    "plt.xlabel('Politeness Level')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores by Politeness Level')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/Users/dell/Yoav/nlp_project/results/BERT_scores/F1_plot.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
